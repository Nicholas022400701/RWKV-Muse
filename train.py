import os
import argparse
import torch
import torch.nn as nn
from torch.optim import AdamW
# from torch.utils.data import DataLoader
# from core.dataset import YourDataset  # è¯·åœ¨è¿™é‡Œå¯¼å…¥ä½ å®é™…çš„æ•°æ®é›†ç±»

from core.architecture import PianoMuseRWKV

def train_epoch(model, dataloader, optimizer, scheduler, device, epoch, grad_clip):
    model.train()
    total_loss = 0.0
    
    for step, batch in enumerate(dataloader):
        # [FIXED] å°†æ•°æ®æ— ç¼é€å…¥ CUDA è®¾å¤‡
        input_ids = batch['input_ids'].to(device, non_blocking=True)
        targets = batch['targets'].to(device, non_blocking=True)
        
        ctx_lengths = batch.get('ctx_lengths', None)
        attention_mask = batch.get('attention_mask', None)
        
        if ctx_lengths is not None:
            ctx_lengths = ctx_lengths.to(device, non_blocking=True)
        if attention_mask is not None:
            attention_mask = attention_mask.to(device, non_blocking=True)
            
        # [FIXED] æ¢å¤ CUDA ç¯å¢ƒä¸‹æœ€ä¼˜é›…çš„æ¢¯åº¦é‡Šæ”¾æ–¹å¼ï¼Œå¼€å¯ set_to_none=True å‹æ¦¨æ˜¾å­˜
        optimizer.zero_grad(set_to_none=True)
        
        # å‰å‘ä¼ æ’­
        logits = model(input_ids, ctx_lengths=ctx_lengths, attention_mask=attention_mask, padding_token_id=0)
        
        # è®¡ç®— Loss
        loss_fct = nn.CrossEntropyLoss(ignore_index=0)
        loss = loss_fct(logits.view(-1, logits.size(-1)), targets.view(-1))
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        if grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            
        # ä¼˜åŒ–å™¨æ­¥è¿›
        optimizer.step()
        if scheduler is not None:
            scheduler.step()
            
        total_loss += loss.item()
        
        if step % 50 == 0:
            print(f"[Epoch {epoch} | Step {step}/{len(dataloader)}] Loss: {loss.item():.4f}")
            
    if len(dataloader) > 0:
        return total_loss / len(dataloader)
    return 0.0

def main(args):
    print("========================================================")
    print("[Genius Protocol] RWKV Piano Muse - NVIDIA CUDA Engine")
    print("========================================================")
    
    # [FIXED] å½»åº•ç§»é™¤ hijack_npu_env()ï¼Œæ‹¥æŠ±çº¯æ­£çš„ NVIDIA CUDA ç”Ÿæ€
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[*] Compute Device: {device}")
    if device.type == 'cuda':
        print(f"[*] GPU Info: {torch.cuda.get_device_name(0)}")
        
    # [FIXED] T4 æ˜¾å¡å…·å¤‡å¼ºå¤§çš„ FP16 Tensor Coresï¼Œä½¿ç”¨ cuda fp16 ç­–ç•¥ç›´æ¥èµ·é£
    print(f"[*] Loading RWKV Model: {args.pretrained_model}")
    model = PianoMuseRWKV(args.pretrained_model, strategy='cuda fp16')
    
    # [FIXED] å½»åº•åˆ é™¤äº† model.parameters() çš„ .contiguous() å¼ºè½¬è¡¥ä¸
    # cuDNN åº•å±‚ä¼šè‡ªåŠ¨å®Œç¾å¤„ç†å†…å­˜åˆ†é…
    model = model.to(device)
    
    # åˆå§‹åŒ–æ•°æ®é›†
    print(f"[*] Loading dataset from: {args.data_path}")
    # dataset = YourDataset(args.data_path)           
    # dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
    dataloader = [] # é˜²æŠ¥é”™å ä½ç¬¦ï¼Œè¯·åˆ é™¤å¹¶æ›¿æ¢ä¸ºçœŸå®çš„ dataloader
    
    # [FIXED] æ¢å› PyTorch åŸç”Ÿçš„é«˜æ•ˆ AdamWï¼ŒæŠ›å¼ƒä¼šæŠ¥é”™çš„ NpuFusedAdamW
    optimizer = AdamW(model.parameters(), lr=args.lr)
    scheduler = None 
    
    print("[*] Training Pipeline Ignited! ğŸ”¥")
    for epoch in range(1, args.epochs + 1):
        if len(dataloader) > 0:
            avg_loss = train_epoch(model, dataloader, optimizer, scheduler, device, epoch, args.grad_clip)
            print(f"==> Epoch {epoch} Complete | Avg Loss: {avg_loss:.4f}\n")
        
        # ä¿å­˜æ¨¡å‹æƒé‡
        os.makedirs(args.output_dir, exist_ok=True)
        save_path = os.path.join(args.output_dir, f"rwkv_muse_epoch_{epoch}.pth")
        torch.save(model.state_dict(), save_path)
        print(f"[*] Checkpoint saved: {save_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # æ³¨æ„ï¼šä¼ ç»™ RWKV å®˜æ–¹åº•å±‚çš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„æ— éœ€å¸¦ .pth åç¼€
    parser.add_argument("--pretrained_model", type=str, default="./models/rwkv_430m")
    parser.add_argument("--data_path", type=str, default="./data/processed/processed_dataset.jsonl")
    parser.add_argument("--output_dir", type=str, default="./outputs")
    parser.add_argument("--batch_size", type=int, default=8) # åœ¨ 16GB æ˜¾å­˜çš„ T4 ä¸Šï¼Œä½ å¯ä»¥å¤§èƒ†æé«˜ Batch Size
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--grad_clip", type=float, default=1.0)
    parser.add_argument("--epochs", type=int, default=10)
    args = parser.parse_args()
    
    main(args)
